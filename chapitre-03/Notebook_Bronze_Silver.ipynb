{"cells":[{"cell_type":"markdown","source":["# Chargement des données dans la zone Silver depuis la zone Bronze"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2d5ba8de-4d89-484a-9a75-2e82f3286d17"},{"cell_type":"markdown","source":["![image-alt-text](https://learn.microsoft.com/en-us/fabric/onelake/media/onelake-medallion-lakehouse-architecture/onelake-medallion-lakehouse-architecture-example.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5261e747-094c-4c70-b8d3-1ad08e07efce"},{"cell_type":"markdown","source":["## Chargement de la table de dimension City\n","En utilisant les bibliothèques PySpark, nous allons définir le schéma des fichiers CSV contenus dans le répertoire \"wwi/full/dimension_city\". Cette définition explicite du schéma optimise les performances du chargement des données. Les données seront lues et écrites dans une table du Lakehouse dans le format Delta Lake."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"92e2d78d-2087-4128-bfe3-f572fab9a8cc"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    'CityKey'                   , IntegerType(), True), \n","        StructField(    'CityID'                    , IntegerType(), True), \n","        StructField(    'City'                      , StringType(), True), \n","        StructField(    'StateProvince'             , StringType(), True), \n","        StructField(    'Country'                   , StringType(), True), \n","        StructField(    'Continent'                 , StringType(), True), \n","        StructField(    'SalesTerritory'            , StringType(), True), \n","        StructField(    'Region'                    , StringType(), True), \n","        StructField(    'SubRegion'                 , StringType(), True), \n","        StructField(    'Location'                  , StringType(), True), \n","        StructField(    'LatestRecordedPopulation'  , LongType(), True), \n","        StructField(    'ValidFrom'                 , TimestampType(), True), \n","        StructField(    'ValidTo'                   , TimestampType(), True), \n","        StructField(    'LineageKey'                , IntegerType(), True)\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_city\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/city\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ab158a21-4602-43d6-bb23-0078db8e019f"},{"cell_type":"markdown","source":["## Chargement de la table de dimension Date\n","En utilisant les bibliothèques PySpark, nous allons définir le schéma des fichiers CSV contenus dans le répertoire \"wwi/full/dimension_date\". Cette définition explicite du schéma optimise les performances du chargement des données. Les données seront lues et écrites dans une table du Lakehouse dans le format Delta Lake."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"44b9d5b5-db5d-4e56-b1ab-96393f7d82cb"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"Date\"                  , TimestampType(), True),\n","        StructField(    \"DayNumber\"             , IntegerType(), True),\n","        StructField(    \"Day\"                   , StringType(), True),\n","        StructField(    \"MonthName\"             , StringType(), True),\n","        StructField(    \"ShortMonthName\"        , StringType(), True),\n","        StructField(    \"CYMonthNumber\"         , IntegerType(), True),\n","        StructField(    \"CYMonthLabel\"          , StringType(), True),\n","        StructField(    \"CYYear\"                , IntegerType(), True),\n","        StructField(    \"CYYearLabel\"           , StringType(), True),\n","        StructField(    \"FYMonthNumber\"         , IntegerType(), True),\n","        StructField(    \"FYMonthLabel\"          , StringType(), True),\n","        StructField(    \"FYYear\"                , IntegerType(), True),\n","        StructField(    \"FYYearLabel\"           , StringType(), True),\n","        StructField(    \"WeekNumber\"            , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_date\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/date\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"715c7d20-047c-430d-943b-72079912615a"},{"cell_type":"markdown","source":["## Chargement de la table de dimension Customer\n","En utilisant les bibliothèques PySpark, nous allons définir le schéma des fichiers CSV contenus dans le répertoire \"wwi/full/dimension_customer\". Cette définition explicite du schéma optimise les performances du chargement des données. Les données seront lues et écrites dans une table du Lakehouse dans le format Delta Lake."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"479f598f-139e-4c24-92e8-9596a353f5f7"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"CustomerKey\"       , IntegerType(), True),\n","        StructField(    \"CustomerID\"        , IntegerType(), True),\n","        StructField(    \"Customer\"          , StringType(), True),\n","        StructField(    \"BillToCustomer\"    , StringType(), True),\n","        StructField(    \"Category\"          , StringType(), True),\n","        StructField(    \"BuyingGroup\"       , StringType(), True),\n","        StructField(    \"PrimaryContact\"    , StringType(), True),\n","        StructField(    \"PostalCode\"        , StringType(), True),\n","        StructField(    \"ValidFrom\"         , TimestampType(), True),\n","        StructField(    \"ValidTo\"           , TimestampType(), True),\n","        StructField(    \"LineageKey\"        , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_customer\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/customer\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5dfd5cfb-ba51-4266-8c35-afc6df94c86c"},{"cell_type":"markdown","source":["## Chargement de la table de dimension Employee\n","En utilisant les bibliothèques PySpark, nous allons définir le schéma des fichiers CSV contenus dans le répertoire \"wwi/full/dimension_employee\". Cette définition explicite du schéma optimise les performances du chargement des données. Les données seront lues et écrites dans une table du Lakehouse dans le format Delta Lake."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ae4063de-17f0-45be-9e46-6a4e051086ab"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"EmployeeKey\"       , IntegerType(), True),\n","        StructField(    \"EmployeeID\"        , IntegerType(), True),\n","        StructField(    \"EmployeeName\"      , StringType(), True),\n","        StructField(    \"PreferredName\"     , StringType(), True),\n","        StructField(    \"IsSalesPerson\"     , BooleanType(), True),\n","        StructField(    \"Photo\"             , StringType(), True),\n","        StructField(    \"ValidFrom\"         , TimestampType(), True),\n","        StructField(    \"ValidTo\"           , TimestampType(), True),\n","        StructField(    \"LineageKey\"        , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_employee\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/employee\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"580c885c-eebb-473a-a865-338e9687f2ae"},{"cell_type":"markdown","source":["## Chargement de la table de dimension StockItem\n","En utilisant les bibliothèques PySpark, nous allons définir le schéma des fichiers CSV contenus dans le répertoire \"wwi/full/dimension_stock_item\". Cette définition explicite du schéma optimise les performances du chargement des données. Les données seront lues et écrites dans une table du Lakehouse dans le format Delta Lake."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6c88b899-4a5a-40ca-b746-c70c51751345"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"StockItemKey\"              , IntegerType(), True),\n","        StructField(    \"StockItemID\"               , IntegerType(), True),\n","        StructField(    \"StockItem\"                 , StringType(), True),\n","        StructField(    \"Color\"                     , StringType(), True),\n","        StructField(    \"SellingPackage\"            , StringType(), True),\n","        StructField(    \"BuyingPackage\"             , StringType(), True),\n","        StructField(    \"Brand\"                     , StringType(), True),\n","        StructField(    \"Size\"                      , StringType(), True),\n","        StructField(    \"LeadTimeDays\"              , IntegerType(), True),\n","        StructField(    \"QuantityPerOuter\"          , IntegerType(), True),\n","        StructField(    \"IsChillerStock\"            , BooleanType(), True),\n","        StructField(    \"Barcode\"                   , StringType(), True),\n","        StructField(    \"TaxRate\"                   , DecimalType(18, 2), True),\n","        StructField(    \"UnitPrice\"                 , DecimalType(18, 2), True),\n","        StructField(    \"RecommendedRetailPrice\"    , DecimalType(18, 2), True),\n","        StructField(    \"WeightPerUnit\"             , DecimalType(18, 2), True),\n","        StructField(    \"Photo\"                     , StringType(), True),\n","        StructField(    \"ValidFrom\"                 , TimestampType(), True),\n","        StructField(    \"ValidTo\"                   , TimestampType(), True),\n","        StructField(    \"LineageKey\"                , IntegerType(), True),\n","    ]   \n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/dimension_stock_item\")\n",")\n","\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/stockitem\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"998b900c-3270-4aab-9474-4773f05873cd"},{"cell_type":"markdown","source":["## Chargement de la table de fait Sales\n","En utilisant les bibliothèques PySpark, nous allons définir le schéma des fichiers CSV contenus dans le répertoire \"wwi/full/dimension_stock_item\". Cette définition explicite du schéma optimise les performances du chargement des données. Les données seront lues et écrites dans une table du Lakehouse dans le format Delta Lake."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6bc50327-0947-4aef-a00d-cedc33a1c17a"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import col, year, month, quarter\n","\n","csv_schema = StructType(\n","    [\n","        StructField(    \"SaleKey\"               , LongType(), True),\n","        StructField(    \"CityKey\"               , IntegerType(), True),\n","        StructField(    \"CustomerKey\"           , IntegerType(), True),\n","        StructField(    \"BillToCustomerKey\"     , IntegerType(), True),\n","        StructField(    \"StockItemKey\"          , IntegerType(), True),\n","        StructField(    \"InvoiceDateKey\"        , TimestampType(), True),\n","        StructField(    \"DeliveryDateKey\"       , TimestampType(), True),\n","        StructField(    \"SalespersonKey\"        , IntegerType(), True),\n","        StructField(    \"InvoiceID\"             , IntegerType(), True),\n","        StructField(    \"Description\"           , StringType(), True),\n","        StructField(    \"Package\"               , StringType(), True),\n","        StructField(    \"Quantity\"              , IntegerType(), True),\n","        StructField(    \"UnitPrice\"             , DecimalType(18, 2), True),\n","        StructField(    \"TaxRate\"               , DecimalType(18, 2), True),\n","        StructField(    \"TotalExcludingTax\"     , DecimalType(18, 2), True),\n","        StructField(    \"TaxAmount\"             , DecimalType(18, 2), True),\n","        StructField(    \"Profit\"                , DecimalType(18, 2), True),\n","        StructField(    \"TotalIncludingTax\"     , DecimalType(18, 2), True),\n","        StructField(    \"TotalDryItems\"         , IntegerType(), True),\n","        StructField(    \"TotalChillerItems\"     , IntegerType(), True),\n","        StructField(    \"LineageKey\"            , IntegerType(), True),\n","    ]\n",")\n","\n","df = (\n","    spark.read.format(\"csv\")\n","    .schema(csv_schema)\n","    .option(\"header\", \"true\")\n","    .load(\"Files/wwi/full/fact_sale\")\n",")\n","\n","df = df.withColumn(\"Year\", year(col(\"InvoiceDateKey\")))\n","df = df.withColumn(\"Quarter\", quarter(col(\"InvoiceDateKey\")))\n","df = df.withColumn(\"Month\", month(col(\"InvoiceDateKey\")))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\", \"Quarter\").save(\"Tables/sales\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7481e33f-4ebc-4355-82a4-bee0f47823be"},{"cell_type":"code","source":["%%sql\n","SELECT Year, Quarter, Month, count(1) AS Sales\n","FROM lakehouse_silver.sales\n","GROUP BY Year, Quarter, Month"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"afb94a19-e840-4758-8ffe-3efca02e1947"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{"default_lakehouse":"3d59019f-91f4-4e3f-9145-de1c38731407","default_lakehouse_name":"lakehouse_silver","default_lakehouse_workspace_id":"a5be6b71-1e6a-48c5-bcf7-22fc2210a9d6"}}},"nbformat":4,"nbformat_minor":5}